{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben Linear Networks\n",
    "========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the common imports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections.abc import Callable\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you should create a linear regression\n",
    "model from scratch and test it on some synthetically created data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(\n",
    "    w: Tensor, b: Tensor, num_examples: int\n",
    ") -> tuple[Tensor, Tensor]:  # @save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "n_samples = 100\n",
    "X, y = synthetic_data(true_w, true_b, n_samples)\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to fit a simple regression model with Batch Gradient Descent.\n",
    "We start with randomly chosen values for the weights and zero bias.\n",
    "First, implement the function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "\n",
    "def linreg(X: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the loss functions to be used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"Compute the sum of the quadratic errors\"\"\"\n",
    "    return (0.5 * (y_hat - y) ** 2).sum() / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the training loop for Gradient Descent.\n",
    "You should not use `autograd` for computing the gradient, instead\n",
    "build on the closed formula presented in the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = torch.zeros(n_epoch)  # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "\n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[i] = squared_loss(y_hat, y)\n",
    "\n",
    "    # 2. Use y_hat and y to compute the gradients\n",
    "    grad_w = ((y_hat - y) * X).mean(axis=0, keepdim=True).T\n",
    "    grad_b = (y_hat - y).T.mean()\n",
    "\n",
    "    # 3. Update the parameters\n",
    "    w -= step * grad_w\n",
    "    b -= step * grad_b\n",
    "\n",
    "plt.semilogy(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear networks with autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to use `autograd` the compute the gradient.\n",
    "You can use the same skeleton as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch)  # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "\n",
    "    loss = squared_loss(y_hat, y)\n",
    "\n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[i] = loss.detach().numpy()\n",
    "\n",
    "    # 2. Use the computed loss to compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 3. Update the parameters, remember to zero the gradients\n",
    "    with torch.no_grad():\n",
    "        w -= step * w.grad\n",
    "        b -= step * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  want to implement a linear network for classification.\n",
    "We use the famous IRIS data set as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the network is implemented as a class,\n",
    "the only thing missing is the implementation of the softmax function,\n",
    "for example\n",
    "$$\n",
    "\\mathrm{softmax}(y_1) = \\frac{e^{y_1}}{ \\sum_{i=1}^p  e^{y_i} }.\n",
    "$$\n",
    "You have to implement it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y: Tensor) -> Tensor:\n",
    "    y_exp = y.exp()\n",
    "    return y_exp / y_exp.sum(axis=1, keepdim=True)\n",
    "\n",
    "\n",
    "class SoftmaxNetwork:\n",
    "    def __init__(self, num_input, num_output, dtype=torch.float64) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_input: dimension of input space\n",
    "            num_output: number of output classes\n",
    "        \"\"\"\n",
    "        self.w = torch.randn((num_input, num_output), dtype=dtype).requires_grad_(True)\n",
    "        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n",
    "\n",
    "    def forward(self, X) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: tensor of shape (n, d)\n",
    "        \"\"\"\n",
    "        y = X @ self.w + self.b\n",
    "        return softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to implement the cross entropy loss, it is already finished:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat: Tensor, y: Tensor) -> Tensor:\n",
    "    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation does not require a one-hot-encoding for $y$\n",
    "(but there is one side effect: $y$ has to be of type `torch.int64`!).\n",
    "\n",
    "The final step is to implement a function that runs the training for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    net: SoftmaxNetwork,\n",
    "    X: Tensor,\n",
    "    y: Tensor,\n",
    "    f_loss: Callable[[Tensor, Tensor], Tensor],\n",
    "    n_epochs: int,\n",
    "    lr=0.1,\n",
    ") -> NDArray:\n",
    "    \"\"\"Run the training.\n",
    "    Args:\n",
    "       net: an instance of SoftmaxNetwork\n",
    "       X, y: training data\n",
    "       f_loss: the loss function\n",
    "       n_epochs: number of epochs\n",
    "       lr: the learning rate\n",
    "\n",
    "    Returns:\n",
    "      training loss: np.array (loss per epoch)\n",
    "    \"\"\"\n",
    "    loss_arr = np.zeros(n_epochs)\n",
    "    for i in range(n_epochs):\n",
    "        net.w.requires_grad_(True)\n",
    "        net.b.requires_grad_(True)\n",
    "        y_hat = net.forward(X)\n",
    "\n",
    "        loss = f_loss(y_hat, y)\n",
    "\n",
    "        loss_arr[i] = loss.detach().numpy()\n",
    "\n",
    "        loss.backward()\n",
    "        net.b.requires_grad_(False)\n",
    "        net.w.requires_grad_(False)\n",
    "\n",
    "        net.w -= lr * net.w.grad\n",
    "        net.b -= lr * net.b.grad\n",
    "        net.w.grad.zero_()\n",
    "        net.b.grad.zero_()\n",
    "\n",
    "    return loss_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model, don't forget to cast X and y to Pytorch tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SoftmaxNetwork(4, 3)\n",
    "train_loss = run_training(\n",
    "    net, torch.from_numpy(X), torch.from_numpy(y), cross_entropy, n_epochs=100, lr=0.1\n",
    ")\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Learning curve\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Run the training several times, and observe the different learning curves.\n",
    "2.  Try the same with a lower learning rate, say $lr=0.05$. Do you see any differences?\n",
    "\n",
    "Finally check the accuracy of the model, that is the fraction of correctly predicted examples.\n",
    "Of course this is on training only. If you like you can try to split the\n",
    "data into train and test and evaluate your network on the test data set.\n",
    "A useful function for this is `train_test_split` found in `sklearn.model_selection`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "org": null,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "033789bd9e7391564cd4bc2754b33d8f54946cdcd3fc1b2b7de7c42567e3bd79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
